apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app: {{ include "hadoop.name" . }}
    chart: {{ include "hadoop.chart" . }}
    release: {{ .Release.Name }}
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml capacity-scheduler.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_PREFIX/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_PREFIX/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      if [ ! -d /root/hdfs/namenode/current ]; then
        mkdir -p /root/hdfs/namenode
        $HADOOP_PREFIX/bin/hdfs namenode -format -force -nonInteractive
      fi
      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      mkdir -p /root/hdfs/datanode

      #  wait up to 30 seconds for namenode
      (while [[ $count -lt 15 && -z `curl -sf http://{{ include "hadoop.fullname" . }}-hdfs-nn:50070` ]]; do ((count=count+1)) ; echo "Waiting for {{ include "hadoop.fullname" . }}-hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start datanode
    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_PREFIX/sbin/
      cd $HADOOP_PREFIX/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://{{ include "hadoop.fullname" . }}-yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for {{ include "hadoop.fullname" . }}-yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_PREFIX}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_PREFIX}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ include "hadoop.fullname" . }}-hdfs-nn:9000/</value>
            <description>NameNode URI</description>
        </property>
        <property>
            <name>hadoop.proxyuser.hue.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.hue.groups</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.C4.hosts</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.C4.groups</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.http.staticuser.user</name>
            <value>root</value>
        </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
    
{{- if .Values.hdfs.webhdfs.enabled -}}
      <property>
          <name>dfs.webhdfs.enabled</name>
          <value>true</value>
      </property> 
{{- end -}}
      
      <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>

      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///root/hdfs/datanode</value>
        <description>DataNode directory</description>
      </property>

      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///root/hdfs/namenode</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>

      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.https-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888</value>
      </property>
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    # "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    #"$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    DEFAULT_LIBEXEC_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start resourceManager
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager
    # start nodeManager
    # "$bin"/yarn-daemons.sh --config $YARN_CONF_DIR  start nodemanager
    # start proxyserver
    "$bin"/yarn-daemon.sh --config $YARN_CONF_DIR  start proxyserver

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle,spark_shuffle</value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
        <value>org.apache.spark.network.yarn.YarnShuffleService</value>
      </property>
      <property>
        <name>spark.shuffle.service.port</name>
        <value>7337</value>
      </property>
      <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /usr/local/hadoop/etc/hadoop,
          /usr/local/hadoop/share/hadoop/common/*,
          /usr/local/hadoop/share/hadoop/common/lib/*,
          /usr/local/hadoop/share/hadoop/hdfs/*,
          /usr/local/hadoop/share/hadoop/hdfs/lib/*,
          /usr/local/hadoop/share/hadoop/mapreduce/*,
          /usr/local/hadoop/share/hadoop/mapreduce/lib/*,
          /usr/local/hadoop/share/hadoop/yarn/*,
          /usr/local/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>

      <property>
        <name>yarn.resourcemanager.fs.state-store.uri</name>
        <value>/rmstate</value>
      </property>
      <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
      </property>
      <property>
        <name>yarn.timeline-service.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</value>
      </property>
      <property>
        <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
        <value>true</value>
      </property>
      <!--property><name>yarn.nodemanager.remote-app-log-dir</name><value>/app-logs</value></property-->
      <!--property><name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value></property-->
      <!--property><name>yarn.resourcemanager.resource.tracker.address</name><value>resourcemanager:8031</value></property-->
      <!--property><name>yarn.resourcemanager.hostname</name><value>resourcemanager</value></property-->
      <property>
        <name>yarn.timeline-service.hostname</name>
        <value>historyserver</value>
      </property>
      <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>98.5</value>
      </property>

    </configuration>


  capacity-scheduler.xml: |
    <configuration>

      <property>
        <name>yarn.scheduler.capacity.maximum-applications</name>
        <value>10000</value>
        <description>
          Maximum number of applications that can be pending and running.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>1</value>
        <description>
          Maximum percent of resources in the cluster which can be used to run
          application masters i.e. controls number of concurrent running
          applications.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
        <description>
          The ResourceCalculator implementation to be used to compare
          Resources in the scheduler.
          The default i.e. DefaultResourceCalculator only uses Memory while
          DominantResourceCalculator uses dominant-resource to compare
          multi-dimensional resources such as Memory, CPU etc.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.queues</name>
        <value>default,dev</value>
        <description>
          The queues at the this level (root is the root queue).
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.capacity</name>
        <value>70</value>
        <description>Default queue target capacity.</description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
        <value>100</value>
        <description>
          The maximum capacity of the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
        <value>2</value>
        <description>
          Default queue user limit a percentage from 0.0 to 1.0.
        </description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.state</name>
        <value>RUNNING</value>
        <description>
          The state of the default queue. State can be one of RUNNING or STOPPED.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
        <value>*</value>
        <description>
          The ACL of who can submit jobs to the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
        <value>*</value>
        <description>
          The ACL of who can administer jobs on the default queue.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.dev.capacity</name>
        <value>30</value>
        <description>dev queue target capacity.</description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>
        <value>60</value>
        <description>
          The maximum capacity of the dev queue.
        </description>
      </property>

    <property>
      <name>yarn.scheduler.capacity.root.dev.user-limit-factor</name>
      <value>2</value>
      <description>
        Default queue user limit a percentage from 0.0 to 1.0.
      </description>
    </property>
    <property>
      <name>yarn.scheduler.capacity.root.dev.state</name>
      <value>RUNNING</value>
      <description>
        The state of the default queue. State can be one of RUNNING or STOPPED.
      </description>
    </property>

    <property>
      <name>yarn.scheduler.capacity.root.dev.acl_submit_applications</name>
      <value>*</value>
      <description>
        The ACL of who can submit jobs to the default queue.
      </description>
    </property>

    <property>
      <name>yarn.scheduler.capacity.root.dev.acl_administer_queue</name>
      <value>*</value>
      <description>
        The ACL of who can administer jobs on the default queue.
      </description>
    </property>

      <property>
        <name>yarn.scheduler.capacity.node-locality-delay</name>
        <value>40</value>
        <description>
          Number of missed scheduling opportunities after which the CapacityScheduler
          attempts to schedule rack-local containers.
          Typically this should be set to number of nodes in the cluster, By default is setting
          approximately number of nodes in one rack which is 40.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.queue-mappings</name>
        <value></value>
        <description>
          A list of mappings that will be used to assign jobs to queues
          The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
          Typically this list will be used to map users to queues,
          for example, u:%user:%user maps all users to queues with the same name
          as the user.
        </description>
      </property>

      <property>
        <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
        <value>false</value>
        <description>
          If a queue mapping is present, will it override the value specified
          by the user? This can be used by administrators to place jobs in queues
          that are different than the one specified by the user.
          The default is false.
        </description>
      </property>

    </configuration>